# Chatbot Base - Docker Compose Configuration
# Supports CPU (default), NVIDIA GPU, and AMD GPU configurations

services:
  # PostgreSQL Database
  db:
    image: postgres:18-trixie
    container_name: chatbot-db
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-chatbot}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-chatbot_dev_password}
      POSTGRES_DB: ${POSTGRES_DB:-chatbot}
      PGDATA: /var/lib/postgresql/18/docker
    volumes:
      - postgres_data:/var/lib/postgresql
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-chatbot} -d ${POSTGRES_DB:-chatbot}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - chatbot-network

  # Ollama LLM Service (CPU)
  # Run with: docker compose --profile default up
  # or: docker compose --profile cpu up
  ollama:
    image: ollama/ollama:latest
    container_name: chatbot-ollama
    restart: unless-stopped
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0
    healthcheck:
      test: ["CMD-SHELL", "ollama list || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 120s
    networks:
      - chatbot-network
    profiles:
      - cpu
      - default

  # Ollama with NVIDIA GPU Support
  # Requires: NVIDIA Container Toolkit (https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)
  # Run with: docker compose --profile nvidia up
  ollama-nvidia:
    image: ollama/ollama:latest
    container_name: chatbot-ollama
    restart: unless-stopped
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "ollama list || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 120s
    networks:
      - chatbot-network
    profiles:
      - nvidia

  # Ollama with AMD GPU Support (ROCm)
  # Requires: AMD ROCm drivers installed on host (https://rocm.docs.amd.com/projects/install-on-linux/en/latest/)
  # Run with: docker compose --profile amd up
  ollama-amd:
    image: ollama/ollama:rocm
    container_name: chatbot-ollama
    restart: unless-stopped
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0
    devices:
      - /dev/kfd
      - /dev/dri
    healthcheck:
      test: ["CMD-SHELL", "ollama list || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 120s
    networks:
      - chatbot-network
    profiles:
      - amd

  # Model Puller for CPU/default profile
  model-puller:
    image: curlimages/curl:latest
    container_name: chatbot-model-puller
    depends_on:
      ollama:
        condition: service_healthy
    entrypoint: >
      sh -c "
        echo 'Waiting for Ollama to be ready...' &&
        sleep 5 &&
        echo 'Pulling development model: ${OLLAMA_MODEL:-llama3.2:3b}...' &&
        curl -X POST http://ollama:11434/api/pull -d '{\"name\": \"${OLLAMA_MODEL:-llama3.2:3b}\"}' &&
        echo 'Model pull initiated successfully'
      "
    networks:
      - chatbot-network
    profiles:
      - cpu
      - default

  # Model Puller for NVIDIA profile
  model-puller-nvidia:
    image: curlimages/curl:latest
    container_name: chatbot-model-puller
    depends_on:
      ollama-nvidia:
        condition: service_healthy
    entrypoint: >
      sh -c "
        echo 'Waiting for Ollama to be ready...' &&
        sleep 5 &&
        echo 'Pulling development model: ${OLLAMA_MODEL:-llama3.2:3b}...' &&
        curl -X POST http://ollama:11434/api/pull -d '{\"name\": \"${OLLAMA_MODEL:-llama3.2:3b}\"}' &&
        echo 'Model pull initiated successfully'
      "
    networks:
      - chatbot-network
    profiles:
      - nvidia

  # Model Puller for AMD profile
  model-puller-amd:
    image: curlimages/curl:latest
    container_name: chatbot-model-puller
    depends_on:
      ollama-amd:
        condition: service_healthy
    entrypoint: >
      sh -c "
        echo 'Waiting for Ollama to be ready...' &&
        sleep 5 &&
        echo 'Pulling development model: ${OLLAMA_MODEL:-llama3.2:3b}...' &&
        curl -X POST http://ollama:11434/api/pull -d '{\"name\": \"${OLLAMA_MODEL:-llama3.2:3b}\"}' &&
        echo 'Model pull initiated successfully'
      "
    networks:
      - chatbot-network
    profiles:
      - amd

  # FastAPI Backend
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile.dev
    container_name: chatbot-backend
    restart: unless-stopped
    depends_on:
      db:
        condition: service_healthy
    environment:
      - DEBUG=${DEBUG:-true}
      - POSTGRES_HOST=db
      - POSTGRES_PORT=5432
      - POSTGRES_USER=${POSTGRES_USER:-chatbot}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-chatbot_dev_password}
      - POSTGRES_DB=${POSTGRES_DB:-chatbot}
      - JWT_SECRET_KEY=${JWT_SECRET_KEY:-dev-secret-key-change-in-production}
      - JWT_ALGORITHM=${JWT_ALGORITHM:-HS256}
      - JWT_ACCESS_TOKEN_EXPIRE_MINUTES=${JWT_ACCESS_TOKEN_EXPIRE_MINUTES:-30}
      - JWT_REFRESH_TOKEN_EXPIRE_DAYS=${JWT_REFRESH_TOKEN_EXPIRE_DAYS:-7}
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.2:3b}
      - ENVIRONMENT=${ENVIRONMENT:-development}
      - CORS_ORIGINS=${CORS_ORIGINS:-["http://localhost:5173","http://localhost:1420","tauri://localhost","https://tauri.localhost","http://tauri.localhost"]}
    volumes:
      - ./backend:/app
    ports:
      - "${BACKEND_PORT:-8000}:8000"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - chatbot-network

volumes:
  postgres_data:
    driver: local
  ollama_data:
    driver: local

networks:
  chatbot-network:
    driver: bridge
